# ğŸ¤– Ask Your Docs â€“ RAG-Powered Document Q&A

Welcome to **Ask Your Docs**, a document-based assistant where users can upload files and instantly query them.  
It uses **hybrid retrieval (FAISS + BM25 + RRF)** and a **Groq LLM backend** to return concise, citation-backed answers â€” no hallucinations.

ğŸ¯ [Live Demo](https://rag-chatbot-assistant.streamlit.app/)  

![Python](https://img.shields.io/badge/Python-3.11+-blue)
![Streamlit](https://img.shields.io/badge/Streamlit-App-orange)
![FAISS](https://img.shields.io/badge/FAISS-VectorStore-green)
![Groq](https://img.shields.io/badge/Groq-LLM-purple)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow)

---

## âœ¨ Features

- ğŸ“„ **Document Upload** â€“ Accepts PDF, Markdown, and TXT  
- ğŸ” **Hybrid Retrieval** â€“ Combines dense embeddings (FAISS) + sparse search (BM25) with Reciprocal Rank Fusion  
- ğŸ“‘ **Verified Answers** â€“ Uses only uploaded docs with inline citations  
- ğŸ’¬ **Chat UI** â€“ Streamlit-based interface for interactive Q&A  
- âš¡ **LLM Backend** â€“ Powered by Groq (Llama-3.1-8B/70B Instant) for low-latency inference  

---

## ğŸ§° Tech Stack

| Tool               | Role |
|--------------------|------|
| **FastEmbed**      | Dense embeddings (`BAAI/bge-small-en-v1.5`) |
| **FAISS**          | Vector similarity search |
| **BM25**           | Sparse keyword retrieval |
| **RRF Fusion**     | Combines dense & sparse hits |
| **Groq LLM**       | Answer generation (via Groq API) |
| **Streamlit**      | Frontend UI |
| **PyPDF2 / Markdown** | Document parsing & chunking |

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ streamlit_app.py           # Streamlit chat interface
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py                # FastAPI entrypoint (REST API)
â”œâ”€â”€ rag_chatbot/
â”‚   â”œâ”€â”€ indexing/              # Document parsing & chunking
â”‚   â”‚   â””â”€â”€ document_processor.py
â”‚   â”œâ”€â”€ retrievers/            # Retrieval logic
â”‚   â”‚   â”œâ”€â”€ bm25_retriever.py
â”‚   â”‚   â””â”€â”€ hybrid_retriever.py
â”‚   â”œâ”€â”€ stores/                # Vector storage
â”‚   â”‚   â””â”€â”€ vector_store.py
â”‚   â”œâ”€â”€ llm/                   # LLM integration
â”‚   â”‚   â””â”€â”€ llm_handler.py
â”‚   â””â”€â”€ pipeline/              # Orchestration (RAG engine)
â”‚       â””â”€â”€ rag_system.py
â”œâ”€â”€ config.py                  # Configs & hyperparams
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Project Highlights

- Designed and implemented a **hybrid RAG pipeline** with Reciprocal Rank Fusion (RRF) to combine FAISS (dense) and BM25 (sparse) retrievals.  
- Built **Streamlit frontend** for interactive Q&A and **FastAPI backend** for REST-based integration.  
- Added **document upload capability** allowing users to query their own PDFs, Markdown, or TXT files.  
- Integrated **Groq LLM (Llama-3.1-8B Instant)** for low-latency, streaming responses.  
- Implemented **citation toggle** to improve UX (answers can be shown with or without sources).  

---

## ğŸ’» Run It Locally

```bash
git clone https://github.com/malindard/rag-chatbot-assistant.git
cd rag-chatbot-assistant
pip install -r requirements.txt
```

Create a `.env` file:

```env
GROQ_API_KEY=your_api_key_here
```

Run the Streamlit app:

```bash
streamlit run streamlit_app.py
```

Or run the API:

```bash
uvicorn api.main:app --reload
```

---

## âš ï¸ Notes

- Answers are **strictly from uploaded documents** â€” no fabrication.  
- Citations appear like:  
  > â€œEmployees are entitled to 12 days of annual leave.â€ [source: policy.pdf Â§Leave Policy]  
- If missing:  
  > â€œNot specified in the provided documents.â€

---

## ğŸ“„ License

MIT License â€” free to use, fork, and extend.  

ğŸ™Œ Contributions are welcome!  
Feel free to **open an issue** for bugs, suggestions, or questions, and send a **pull request** if you'd like to improve the project.  